# MNIST-FCNN
Лабораторная работа, цель которой - изучить метод обратного распространения ошибки на примере задачи классификации цифр из тестового набора [MNIST](http://yann.lecun.com/exdb/mnist/ "MNIST dataset") с использованием полносвязной нейронной сети с одним скрытым слоем. 

## Описание сети
- На вход подается одноканальное изображение цифры 28х28 пикселей.
- Количество нейронов на скрытом слое задается параметром командной строки.
- На выходном слое 10 нейронов.
- В качестве функции активации на скрытом слое используется гиперболический тангенс, а на выходном - softmax.
- Функция потерь - cross-entropy.

## Теория
Теория находится в папке *theory*.

## Запуск
В папке с проектом находится скрипт *mnist.py*, который загрузит тренеровочные и тестовые данные в папку */mnist*. Запуск программы производится со следующими параметрами:

```
mnist-fcnn [путь до MNIST] [количество эпох] [скорость обучения] [кол-во нейронов на скрытом слое] [размер одного "пакета"] [кол-во эл-ов обучающей выборки]
```
Следующие параметры являются параметрами по умолчанию:
```
mnist-fcnn mnist 10 0.2 100 100 60000
```
Пример обучения сети со 120 нейронами на скрытом слое в течение 10 эпох со скоростью 0.1 пачками по 30 изображений на всём тренеровочном множестве (60000 элементов):
```
mnist-fcnn "D:\dev\mnist" 10 0.1 120 30
```