# MNIST-FCNN
Лабораторная работа, цель которой - изучить метод обратного распространения ошибки на примере задачи классификации цифр из тестового набора [MNIST](http://yann.lecun.com/exdb/mnist/ "MNIST dataset") с использованием полносвязной нейронной сети с одним скрытым слоем. 

## Описание сети
- На вход подается одноканальное изображение цифры 28х28 пикселей.
- Количество нейронов на скрытом слое задается параметром командной строки.
- На выходном слое 10 нейронов.
- В качестве функции активации на скрытом слое используется гиперболический тангенс, а на выходном - softmax.
- Функция потерь - cross-entropy.

## Теория
Теория находится в папке *theory*.

## Зависимости проекта
- Поддержка стандарта ISO C++17.
- OpenMP

## Запуск
В папке с проектом находится скрипт *mnist.py*, который загрузит тренеровочные и тестовые данные в папку *mnist*. Запуск программы производится со следующими параметрами:

```
mnist-fcnn [путь до MNIST] [количество эпох] [скорость обучения] [кол-во нейронов на скрытом слое] [размер одного "пакета"] [кол-во эл-ов обучающей выборки]
```
Параметры по умолчанию:
```
mnist-fcnn mnist 10 0.2 100 100 60000
```
Пример обучения сети со 120 нейронами на скрытом слое в течение 10 эпох со скоростью 0.1 пачками по 30 изображений на всём тренеровочном множестве (60000 элементов):
```
mnist-fcnn "D:\dev\mnist" 10 0.1 120 30
```

## Тестирование
- Windows 10 x64
- процессор Intel Core i5-6600K, 4 ядра, разогнанные до 4.3ГГц
- оперативная память 2x DIMM DDR4, 8ГБ, 2133МГц

Результаты тестов находятся в файле *test.txt*

Оптимальные параметры обучения сети:
```
Epoch count: 20
Learning rate: 0.3
Hidden layer size: 100
Batch size: 30
```
На 9-ой эпохе потери сети = 0.1030, а точность = 0.9711.

P.S. Полная повторяемость результатов недостижима всвязи с использованием многопоточности.


